# ğŸš¢ Kaggle Competition: [Titanic - Machine Learning from Disaster]  

[![Kaggle Competition Badge](https://img.shields.io/badge/Titanic-20BEFF.svg)](https://www.kaggle.com/competitions/titanic)  
[![License](https://img.shields.io/github/license/1AyaNabil1/Kaggle-Competition.svg)](https://github.com/1AyaNabil1/Kaggle-Competition/blob/main/LICENSE)  

## ğŸ† Predicting Survival on the Titanic Kaggle Competition  
This directory contains the code and steps to predict passenger survival in the Titanic dataset for the Kaggle competition. The code achieves a **0.76555 score**. ğŸš¢

---

## ğŸ“Œ Introduction  
This directory provides the complete workflow for predicting passenger survival in the Titanic disaster using machine learning. The goal is to leverage various passenger features to build an accurate model. This README outlines data analysis, feature engineering, and the machine learning approach used.

## ğŸ“– Table of Contents  
- [ğŸš¢ Kaggle Competition: \[Titanic - Machine Learning from Disaster\]](#-kaggle-competition-titanic---machine-learning-from-disaster)
  - [ğŸ† Predicting Survival on the Titanic Kaggle Competition](#-predicting-survival-on-the-titanic-kaggle-competition)
  - [ğŸ“Œ Introduction](#-introduction)
  - [ğŸ“– Table of Contents](#-table-of-contents)
  - [1ï¸âƒ£ ğŸ“Œ Introduction ](#1ï¸âƒ£--introduction-)
  - [2ï¸âƒ£ âš™ï¸ Installation ](#2ï¸âƒ£-ï¸-installation-)
  - [3ï¸âƒ£ ğŸ“Š Data Preparation ](#3ï¸âƒ£--data-preparation-)
  - [4ï¸âƒ£ ğŸ“ˆ Exploratory Data Analysis ](#4ï¸âƒ£--exploratory-data-analysis-)
  - [5ï¸âƒ£ ğŸ›  Feature Engineering ](#5ï¸âƒ£--feature-engineering-)
  - [6ï¸âƒ£ ğŸ¤– Machine Learning ](#6ï¸âƒ£--machine-learning-)
  - [7ï¸âƒ£ ğŸ“¡ Modeling ](#7ï¸âƒ£--modeling-)
  - [8ï¸âƒ£ ğŸ“‰ Evaluation ](#8ï¸âƒ£--evaluation-)
  - [9ï¸âƒ£ ğŸ” Feature Importance ](#9ï¸âƒ£--feature-importance-)
  - [ğŸ”Ÿ ğŸ“¤ Submission ](#--submission-)

---

## 1ï¸âƒ£ ğŸ“Œ Introduction <a name="introduction"></a>  
This Kaggle competition involves predicting passenger survival in the **Titanic disaster** using demographic and ticket information. The objective is to develop a precise model that improves our understanding of survival factors. ğŸš¢

---

## 2ï¸âƒ£ âš™ï¸ Installation <a name="installation"></a>  
Before running the code, install the required libraries:  
```bash
pip install numpy pandas matplotlib seaborn scikit-learn xgboost
```

---

## 3ï¸âƒ£ ğŸ“Š Data Preparation <a name="data-preparation"></a>  
Load the datasets:  
```python
import numpy as np
import pandas as pd 

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
train['train_test'] = 1
test['train_test'] = 0
test['Survived'] = np.NaN
df = pd.concat([train, test])
```
Check for missing values and data types:  
```python
df.isnull().sum()
df.info()
```

---

## 4ï¸âƒ£ ğŸ“ˆ Exploratory Data Analysis <a name="exploratory-data-analysis"></a>  
Gain insights into survival distribution and trends:
âœ… Visualize **survival rates by passenger class & gender** ğŸ­  
âœ… Analyze the impact of **age & family size** ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦  
âœ… Explore **ticket price & cabin assignment effects** ğŸ’°  

---

## 5ï¸âƒ£ ğŸ›  Feature Engineering <a name="feature-engineering"></a>  
ğŸ”¹ Extract titles from passenger names (e.g., Mr., Miss, Master) ğŸ·ï¸  
ğŸ”¹ Create new features such as **family size** and **deck location** ğŸš¢  
ğŸ”¹ Normalize **Fare** and **Age** values ğŸ“  

---

## 6ï¸âƒ£ ğŸ¤– Machine Learning <a name="machine-learning"></a>  
ğŸ“Œ **Key steps in model development:**  
ğŸ”¹ Feature selection ğŸ¯  
ğŸ”¹ **Handling categorical variables** with encoding ğŸ·ï¸  
ğŸ”¹ Training **multiple models**: Logistic Regression, Decision Trees, Random Forest, XGBoost ğŸš€  
ğŸ”¹ **Evaluating** performance using accuracy & F1-score ğŸ“Š  

---

## 7ï¸âƒ£ ğŸ“¡ Modeling <a name="modeling"></a>  
Define **features & target variable**, split the data, and train the model using XGBoost:  
```python
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier

X, y = train.drop(columns=['Survived']), train['Survived']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', XGBClassifier(
        # Model hyperparameters
    ))
])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
```

---

## 8ï¸âƒ£ ğŸ“‰ Evaluation <a name="evaluation"></a>  
Evaluate model performance using **accuracy, precision, recall, and F1-score** ğŸ“ˆ.  

---

## 9ï¸âƒ£ ğŸ” Feature Importance <a name="feature-importance"></a>  
Identify the most influential features in predicting **survival on the Titanic** ğŸ§.  

---

## ğŸ”Ÿ ğŸ“¤ Submission <a name="submission"></a>  
Generate predictions for the test dataset and prepare the submission file:  
```python
submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': pipeline.predict(test)})
submission.to_csv('submission.csv', index=False)
```

---

ğŸš€ **Let's achieve top ranks on the Kaggle leaderboard!** ğŸ†
